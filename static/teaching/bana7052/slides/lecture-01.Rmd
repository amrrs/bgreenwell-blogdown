---
title: "Association and Simple Linear Regression"
subtitle: "Lecture 01"
author: "Brandon M. Greenwell"
date: "`r format(Sys.time(), '%d %B, %Y')`"
output:
  xaringan::moon_reader:
    css: ["default", "custom.css", "hygge"]
    lib_dir: libs
    nature:
      highlightStyle: github
      highlightLines: true
      countIncrementalSlides: false
---

```{r setup, include=FALSE}
options(htmltools.dir.version = FALSE, servr.daemon = TRUE)

# Global chunk options
knitr::opts_chunk$set(
  echo = TRUE,
  # dev = "svg",
  fig.align = "center",
  fig.width = 6,
  fig.asp = 0.618,
  out.width = "70%",
  message = FALSE,
  warning = FALSE,
  error = FALSE
)

# Load required packages
library(dplyr)
library(patchwork)
```

background-image: url(https://creativemaths.net/wp-content/uploads/2012/03/it_is_so_easy1-704x480.png)

???


---
class: inverse

# Reading assignment

.larger[

* Chapter: 1

* Sections: 1.1--1.8

* Main topics:

    - Association `r set.seed(101); emo::ji("sleep")`
    
    - Simple linear regression
  
]


---

# Prerquisites<sup>1</sup>

.scrollable[

```{r prerequisites, eval=FALSE}
# List of required (CRAN) packages
pkgs <- c(
  "dplyr",    # for data wrangling
  "ggplot2",  # for awesome graphics
  "tibble"    # for nicer data frames
)

# Install required (CRAN) packages
for (pkg in pkgs) {
  if (!requireNamespace(pkg)) {  # check if already installed first
    install.packages(pkg)  # install it
  }
}

# Install additional (optional) awesomeness
install.packages(c("devtools", "magick"))
devtools::install_github("bgreenwell/roundhouse")
```

]

.footnote[
[1] I apologize now for the excessive amount of Chuck Norris gifs you will be subject to.
]


---

# Ready to begin?

--

```{r roundhouse-01}
roundhouse::kick("Chuck Norris knows the last digit of pi", 
                 width = 50)
```


---

# Read the `r emo::ji("book")`!

```{r wonka, echo=FALSE, out.width="50%"}
knitr::include_graphics("images/read-the-book.jpg")
```


---

# Statistical relationships

.scrollable[

```{r statistical-relationships-01, eval=FALSE}
# Load required packages
library(ggplot2)
library(tibble)

# Generate some random data
set.seed(101)
d <- tibble(
  x = seq(from = -2, to = 2, length = 100),
  y1 = 1 + x + rnorm(length(x)),
  y2 = 1 + x^2 + rnorm(length(x)),
  y3 = 1 + x^3 + rnorm(length(x)),
  y4 = 1 + rnorm(length(x))
)

# Construct scatterplots
p1 <- ggplot(d, aes(x = x, y = y1)) +
  geom_point(alpha = 0.5) + 
  labs(y = "y", title = "Linear")
p2 <- ggplot(d, aes(x = x, y = y2)) +
  geom_point(alpha = 0.5) + 
  labs(y = "y", title = "Quadratic")
p3 <- ggplot(d, aes(x = x, y = y3)) +
  geom_point(alpha = 0.5) + 
  labs(y = "y", title = "Cubic")
p4 <- ggplot(d, aes(x = x, y = y4)) +
  geom_point(alpha = 0.5) + 
  labs(y = "y", title = "None")

# Display plots in a grid
gridExtra::grid.arrange(p1, p2, p3, p4, ncol = 2)
```

]


---

# Statistical relationships

```{r statistical-relationships-02, echo=FALSE, out.width="100%"}
# Load required packages
library(ggplot2)
library(tibble)

# Set ggplot2 theme
theme_set(theme_light())

# Generate some random data
set.seed(101)
d <- tibble(
  x = seq(from = -2, to = 2, length = 100),
  y1 = 1 + x + rnorm(length(x)),
  y2 = 1 + x^2 + rnorm(length(x)),
  y3 = 1 + x^3 + rnorm(length(x)),
  y4 = 1 + rnorm(length(x))
)

# Construct scatterplots
p1 <- ggplot(d, aes(x = x, y = y1)) +
  geom_point(alpha = 0.5) + 
  labs(y = "y", title = "Linear")
p2 <- ggplot(d, aes(x = x, y = y2)) +
  geom_point(alpha = 0.5) + 
  labs(y = "y", title = "Quadratic")
p3 <- ggplot(d, aes(x = x, y = y3)) +
  geom_point(alpha = 0.5) + 
  labs(y = "y", title = "Cubic")
p4 <- ggplot(d, aes(x = x, y = y4)) +
  geom_point(alpha = 0.5) + 
  labs(y = "y", title = "None")

# Display plots in a grid
gridExtra::grid.arrange(p1, p2, p3, p4, ncol = 2)
```


---

# Pearson correlation

```{r pearson-correlation-01, echo=FALSE, out.width="100%"}
set.seed(101)
d <- tibble(
  x = seq(from = -2, to = 2, length = 100),
  y1 = 1 + x + rnorm(length(x)),
  y2 = 1 - x + rnorm(length(x)),
  y3 = 1 + x^2 + rnorm(length(x)),
  y4 = 1 + rnorm(length(x))
)
p1 <- ggplot(d, aes(x = x, y = y1)) +
  geom_point(alpha = 0.75) + 
  labs(y = "y", title = "Positive")
p2 <- ggplot(d, aes(x = x, y = y2)) +
  geom_point(alpha = 0.75) + 
  labs(y = "y", title = "Negative")
p3 <- ggplot(d, aes(x = x, y = y4)) +
  geom_point(alpha = 0.75) + 
  labs(y = "y", title = "None")
p4 <- "images/correlation-coefficient.png" %>%
  png::readPNG() %>%
  grid::rasterGrob(interpolate = TRUE, width = 0.8)
p4 <- ggplot() +
  annotation_custom(p4, xmin = -Inf, xmax = Inf, ymin = -Inf, ymax = Inf)
# p1 + p2 + p3 - p4 + plot_layout(ncol = 1, widths = c(1, 1, 1), heights = c(1, 1))
(p1 | p2 | p3) / p4
```


---

# Pearson correlation

```{r pearson-correlation-02, echo=FALSE}
ggplot(d, aes(x = x, y = y3)) +
  geom_point(alpha = 0.5) + 
  labs(y = "y", title = "?")
```


---

# Pearson correlation

* The (Pearson) correlation coefficient (also called the *Pearson product-moment correlation coefficient*) between two random variables $X$ and $Y$ is given by $Cor\left(X, Y\right) = \rho = Cov\left(X, Y\right) / \left(\sigma_X\sigma_Y\right)$

* Given a sample $\left\{\left(x_i, y_i\right)\right\}_{i=1}^n$, we estimate $\rho$ with $r = S_{xy} / \sqrt{S_{xx}S_{yy}}$

  - Where, for example, $S_{xy} = \sum_{i=1}^n\left(x_i - \bar{x}\right)\left(y_i - \bar{y}\right)$


---

# Pearson correlation

* It is common to test the hypothesis $H_0: \rho = 0$ vs. $H_1: \rho \ne 0$

    - Rejecting $H_0$ is only evidence that $\rho$ is **not exactly zero**

    - A *p*-value **does not measure the magnitude of the (linear) association**

    - Sample size affects the p-value! `r emo::ji("scream")`

* There is a formal relationship between Pearson's correlation and the *simple linear regression* (SLR) model

    - Simple linear relationships can be described by a *slope* and *intercept*


---

# Cautions in interpretting correlation(s)

* Correlation between $X$ and $Y$ does not imply causation

    - It does not imply the direction of any possible **causal relationship** between $X$ and $Y$
    
    - There might be a third (*lurking*) variable that is the cause of changes in both variables (i.e., the association between $X$ and $Y$ might be indirect)
    
* The Pearson correlation only measures the **linear association **between $X$ and $Y$


---

# Correlation does NOT imply causation

* [Correlation and uncontrolled experiments](https://imgur.com/gallery/uBSTj2a)

* [Other examples](http://www.tylervigen.com/spurious-correlations)


---

```{r correlation-causation, echo=FALSE, fig.cap="If you get the joke in this comic, then you probably undestand enough about correlation!"}
knitr::include_graphics("images/correlation-causation-comic.png")
```


---

# Which of the following statement(s) is/are true?

a. The Pearson correlation coefficient is a measure of linear association

b. A non-significant *p*-value for a Pearson correlation means no relationship

c. A negative Pearson correlation indicates a low degree of linear association

d. A random cloud of data implies a negative correlation.


---

# Which of the following statement(s) is/are true?

a. **The Pearson correlation coefficient is a measure of linear association**

b. A non-significant *p*-value for a Pearson correlation means no relationship

c. A negative Pearson correlation indicates a low degree of linear association

d. A random cloud of data implies a negative correlation.


---
class: inverse, center, middle

### "All models are wrong, but some are useful."

```{r george-box, echo=FALSE, fig.cap="George E. Box", out.width="40%"}
knitr::include_graphics("images/george-box.jpg")
```


---

# Historical origins of regression

* First developed by Sir Francis Galton in the later part of the 19th century

* Studied heights of parents and their children

* Noted that the heights of children with tall/short parents tended to "revert" or "regress" to the mean of the group

* The term "regression" persists to this day!


---

# Historical origins of regression

```{r galton-01, eval=FALSE}
install.packages(c("ggplot2", "HistData"))
library(ggplot2)
ggplot(HistData::Galton, aes(x = parent, y = child)) +
  geom_point(alpha = 0.5) +
  geom_smooth(method = "lm", formula = y ~ x, se = FALSE) +
  geom_abline(intercept = 0, slope = 1, linetype = "dashed") + 
  labs(x = "Height of mid-parent (in)", y = "Height of child (in)")
```


---

# Historical origins of regression

```{r galton-02, echo=FALSE}
ggplot(HistData::Galton, aes(x = parent, y = child)) +
  geom_point(alpha = 0.5) +
  geom_smooth(method = "lm", formula = y ~ x, se = FALSE) +
  geom_abline(intercept = 0, slope = 1, linetype = "dashed") + 
  labs(x = "Height of mid-parent (in)", y = "Height of child (in)")
```


---

# Functional relationships

```{r functional-relation, echo=FALSE}
ggplot(data.frame(x = 1:10, y = 15*1:10), aes(x, y)) +
  geom_line() +
  geom_point(size = 2, color = "red2") +
  labs(x = "Hours worked (X)", y = "Rate of pay (Y)", 
       title = expression(paste("Relation: ", Y == 15 %*% X)))
```


---

# Statistical relationships

```{r linear-relationships, echo=FALSE, fig.width=7, fig.height=3.5}
p1 <- ggplot(investr::crystal, aes(x = time, y = weight)) +
  geom_point() +
  labs(x = "Time (hours)", 
       y = "Weight (grams)", 
       title = "Crystal weight data")
p2 <- ggplot(investr::arsenic, aes(x = actual, y = measured)) +
  geom_point() +
  labs(x = "True amount of arsenic", 
       y = "Measured amount of arsenic",
       title = "Arsenic concentration data")
gridExtra::grid.arrange(p1, p2, nrow = 1)
```


---

# Examples of statistical relationships

* Simple linear regression (SLR)

    - $Y = \beta_0 + \beta_1 X + \epsilon$
    
* Multiple linear regression (MLR)

    - $Y = \beta_0 + \beta_1 X_1 + \beta_2 X_2 + \dots + \beta_p X_p + \epsilon$
    
    - $Y = \beta_0 + \sum_{i=1}^p \beta_p X_p + \epsilon$
    
 * Polynomial regression (of degree $p$)

    - $Y = \beta_0 + \sum_{i=1}^p \beta_p X^p + \epsilon$   

 * Nonlinear regression

    - $Y = \frac{\beta_1 X}{\beta_2 + X} + \epsilon$   


---

# Still with me?

--

```{r roundhouse-02}
roundhouse::kick("Chuck Norris caught all the Pok\uE9mon from a landline",
                 width = 40, type = 2, fps = 10)
```


---

# Simple linear regression (SLR)

* Data: $\left\{\left(X_i, Y_i\right)\right\}_{i=1}^n$

* Model: $Y_i = \beta_0 + \beta_1 X_i + \epsilon_i$

    - $Y_i$ is a continuous response

    - $X_i$ is a continuous predictor

    - $\beta_0$  is the intercept of the regression line (also called the *bias term*)

    - $\beta_1$  is the slope of the regression line
    
    - $\epsilon_i \sim iid \left(0, \sigma^2\right)$


---

# Assumptions about the errors

* For $i/j = 1, 2, \dots, n$

    1. $\quad E\left(\epsilon_i\right) = 0$

    2. $\quad Var\left(\epsilon_i\right) = \sigma^2$ (homoscedacticity `r emo::ji("scream")`)

    3. $\quad Cov\left(\epsilon_i, \epsilon_j\right) = 0$ (independence)
    
* Assumptions 1.--3. can be summarized as $\epsilon_i \sim iid \left(0, \sigma^2\right)$
    

---

# Properites of SLR

* Simple linear regression: $Y_i = \beta_0 + \beta_1 X_i + \epsilon_i$

    - Implies the model is **linear in the regression coefficients $\beta_0$ and $\beta_1$**
    
* The error term is a random variable; hence, $Y_i$ is also a random variable (**Why?** `r emo::ji("thinking")`)

    - What is $E\left(Y_i|X_i\right)$ and $Var\left(Y_i|X_i\right)$?
    
* $Cor\left(Y_i, Y_j\right) = 0$ $\forall i \ne j$ (**Why?** `r emo::ji("thinking")`)


---

# The SLR model with normal errors

$$
Y_i = \beta_0 + \beta_1 X_i + \epsilon_i, \quad i = 1, 2, \dots, n
$$

<br/>

where $\epsilon_i \stackrel{iid}{\sim} N\left(0, \sigma^2\right)$, is equivalent to

<br/>

$$
Y_i \stackrel{iid}{\sim} N\left(\beta_0 + \beta_1 X_i, \sigma^2\right), \quad i = 1, 2, \dots, n
$$

* $\mu_i = \beta_0 + \beta_1 X_i$ is called the *linear predictor* (LP)


---

# Interpretting the regression coefficients

* $\beta_0$ is the *y*-intercept (or *bias term*)

    - It represent the mean response when $X = 0$; that is $\beta_0 = E\left(Y|X = 0\right)$

    - In general, the intercept is of little practical interest (this is especially true in MLR or when zero is not a valid value of $X$)

* $\beta_1$ is the slope of the regression line

    - The slope of a line represents a *rate of change*
    
    - It represents the change in the mean response per one-unit increase in $X$


---

```{r slr-conditional-distribution, echo=FALSE}
knitr::include_graphics("images/slr-conditional-distribution.png")
```


---

```{r arsenic-conditional-distribution, echo=FALSE}
ggplot(investr::arsenic, aes(x = actual, y = measured)) + 
  geom_point(alpha = 0.75) +
  geom_smooth(method = "lm", se = FALSE, size = 0.5) + 
  labs(x = "Actual amount (ppm)", "Measured amount (ppm)",
       title = "Arsenic concentration data")
```
